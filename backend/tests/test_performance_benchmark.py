"""
æ€§èƒ½åŸºå‡†æµ‹è¯•
æµ‹è¯•ç³»ç»Ÿå„ç»„ä»¶çš„å“åº”æ—¶é—´ã€ååé‡å’Œèµ„æºä½¿ç”¨
"""

import asyncio
import time
import pytest
from typing import List
from datetime import datetime
from unittest.mock import Mock, AsyncMock

from src.core.memory import (
    SemanticMemoryService,
    EnhancedEpisodicMemoryService,
    WorkingMemoryService,
    MultiTierMemoryRetrieval
)
from src.core.evolution import (
    FeedbackPipeline,
    ExperienceExtractor,
    PolicyOptimizer
)
from src.services.cache_service import CacheService


# ========== æ€§èƒ½æŒ‡æ ‡ ==========


class PerformanceMetrics:
    """æ€§èƒ½æŒ‡æ ‡æ”¶é›†å™¨"""

    def __init__(self, name: str):
        self.name = name
        self.durations: List[float] = []
        self.success_count = 0
        self.error_count = 0

    def record(self, duration: float, success: bool = True):
        """è®°å½•ä¸€æ¬¡æ“ä½œ"""
        self.durations.append(duration)
        if success:
            self.success_count += 1
        else:
            self.error_count += 1

    def get_stats(self) -> dict:
        """è·å–ç»Ÿè®¡ä¿¡æ¯"""
        if not self.durations:
            return {
                "name": self.name,
                "count": 0,
                "avg": 0,
                "min": 0,
                "max": 0,
                "p50": 0,
                "p95": 0,
                "p99": 0,
                "success_rate": 0
            }

        sorted_durations = sorted(self.durations)
        count = len(self.durations)

        return {
            "name": self.name,
            "count": count,
            "avg": sum(self.durations) / count,
            "min": sorted_durations[0],
            "max": sorted_durations[-1],
            "p50": sorted_durations[int(count * 0.5)],
            "p95": sorted_durations[int(count * 0.95)],
            "p99": sorted_durations[int(count * 0.99)],
            "success_rate": self.success_count / count if count > 0 else 0
        }


def measure_performance(metrics: PerformanceMetrics):
    """æ€§èƒ½æµ‹é‡è£…é¥°å™¨"""
    def decorator(func):
        async def wrapper(*args, **kwargs):
            start = time.time()
            try:
                result = await func(*args, **kwargs)
                duration = time.time() - start
                metrics.record(duration, success=True)
                return result
            except Exception as e:
                duration = time.time() - start
                metrics.record(duration, success=False)
                raise e
        return wrapper
    return decorator


# ========== ç¼“å­˜æ€§èƒ½æµ‹è¯• ==========


@pytest.mark.asyncio
class TestCachePerformance:
    """ç¼“å­˜æ€§èƒ½æµ‹è¯•"""

    @pytest.fixture
    async def cache_service(self):
        """åˆ›å»ºç¼“å­˜æœåŠ¡"""
        cache = CacheService(enable_l1=True, enable_l2=False)  # ç¦ç”¨L2é¿å…ä¾èµ–
        return cache

    async def test_cache_write_performance(self, cache_service):
        """æµ‹è¯•ç¼“å­˜å†™å…¥æ€§èƒ½"""
        metrics = PerformanceMetrics("cache_write")

        # æ‰§è¡Œ100æ¬¡å†™å…¥
        for i in range(100):
            @measure_performance(metrics)
            async def write_op():
                await cache_service.set(
                    "test",
                    f"key-{i}",
                    {"data": f"value-{i}"},
                    ttl=60
                )
            await write_op()

        stats = metrics.get_stats()
        print(f"\nğŸ“Š ç¼“å­˜å†™å…¥æ€§èƒ½:")
        print(f"  å¹³å‡: {stats['avg']*1000:.2f}ms")
        print(f"  P95: {stats['p95']*1000:.2f}ms")
        print(f"  P99: {stats['p99']*1000:.2f}ms")

        # æ–­è¨€ï¼šå¹³å‡å†™å…¥æ—¶é—´åº”è¯¥ < 1ms
        assert stats['avg'] < 0.001
        print("  âœ… å†™å…¥æ€§èƒ½æµ‹è¯•é€šè¿‡")

    async def test_cache_read_performance(self, cache_service):
        """æµ‹è¯•ç¼“å­˜è¯»å–æ€§èƒ½"""
        # å…ˆå†™å…¥ä¸€äº›æ•°æ®
        for i in range(100):
            await cache_service.set("test", f"key-{i}", {"data": f"value-{i}"})

        metrics = PerformanceMetrics("cache_read")

        # æ‰§è¡Œ100æ¬¡è¯»å–
        for i in range(100):
            @measure_performance(metrics)
            async def read_op():
                await cache_service.get("test", f"key-{i}")
            await read_op()

        stats = metrics.get_stats()
        print(f"\nğŸ“Š ç¼“å­˜è¯»å–æ€§èƒ½:")
        print(f"  å¹³å‡: {stats['avg']*1000:.2f}ms")
        print(f"  P95: {stats['p95']*1000:.2f}ms")
        print(f"  P99: {stats['p99']*1000:.2f}ms")

        # æ–­è¨€ï¼šå¹³å‡è¯»å–æ—¶é—´åº”è¯¥ < 0.5ms (L1ç¼“å­˜)
        assert stats['avg'] < 0.0005
        print("  âœ… è¯»å–æ€§èƒ½æµ‹è¯•é€šè¿‡")


# ========== è®°å¿†ç³»ç»Ÿæ€§èƒ½æµ‹è¯• ==========


@pytest.mark.asyncio
class TestMemoryPerformance:
    """è®°å¿†ç³»ç»Ÿæ€§èƒ½æµ‹è¯•"""

    async def test_semantic_memory_add_performance(self):
        """æµ‹è¯•è¯­ä¹‰è®°å¿†æ·»åŠ æ€§èƒ½"""
        mock_vector_store = Mock()
        mock_vector_store.add_documents = AsyncMock(return_value=100)

        semantic = SemanticMemoryService(mock_vector_store, Mock())

        metrics = PerformanceMetrics("semantic_add")

        # æ·»åŠ 100æ¡çŸ¥è¯†
        for i in range(100):
            @measure_performance(metrics)
            async def add_op():
                await semantic.add_knowledge(
                    knowledge_type="statute",
                    title=f"æµ‹è¯•æ³•å¾‹æ¡æ–‡ {i}",
                    content=f"è¿™æ˜¯ç¬¬ {i} æ¡æµ‹è¯•æ³•å¾‹å†…å®¹"
                )
            await add_op()

        stats = metrics.get_stats()
        print(f"\nğŸ“Š è¯­ä¹‰è®°å¿†æ·»åŠ æ€§èƒ½:")
        print(f"  å¹³å‡: {stats['avg']*1000:.2f}ms")
        print(f"  P95: {stats['p95']*1000:.2f}ms")

        # æ–­è¨€ï¼šå¹³å‡æ·»åŠ æ—¶é—´åº”è¯¥ < 100ms
        assert stats['avg'] < 0.1
        print("  âœ… è¯­ä¹‰è®°å¿†æ·»åŠ æ€§èƒ½æµ‹è¯•é€šè¿‡")

    async def test_episodic_memory_add_performance(self):
        """æµ‹è¯•æƒ…æ™¯è®°å¿†æ·»åŠ æ€§èƒ½"""
        mock_vector_store = Mock()
        mock_vector_store.add_documents = AsyncMock(return_value=1)

        episodic = EnhancedEpisodicMemoryService(mock_vector_store, Mock())

        metrics = PerformanceMetrics("episodic_add")

        # æ·»åŠ 100æ¡æƒ…æ™¯
        for i in range(100):
            @measure_performance(metrics)
            async def add_op():
                await episodic.add_episode(
                    session_id=f"session-{i}",
                    task_description=f"æµ‹è¯•ä»»åŠ¡ {i}",
                    task_type="test",
                    agents_involved=["TestAgent"],
                    execution_trace={"agent_sequence": ["TestAgent"]},
                    result_summary="æµ‹è¯•ç»“æœ",
                    user_rating=5
                )
            await add_op()

        stats = metrics.get_stats()
        print(f"\nğŸ“Š æƒ…æ™¯è®°å¿†æ·»åŠ æ€§èƒ½:")
        print(f"  å¹³å‡: {stats['avg']*1000:.2f}ms")
        print(f"  P95: {stats['p95']*1000:.2f}ms")

        # æ–­è¨€ï¼šå¹³å‡æ·»åŠ æ—¶é—´åº”è¯¥ < 150ms
        assert stats['avg'] < 0.15
        print("  âœ… æƒ…æ™¯è®°å¿†æ·»åŠ æ€§èƒ½æµ‹è¯•é€šè¿‡")

    async def test_multi_tier_retrieval_performance(self):
        """æµ‹è¯•è·¨å±‚æ£€ç´¢æ€§èƒ½"""
        mock_vector_store = Mock()
        mock_vector_store.search = AsyncMock(return_value=[])

        mock_db = Mock()

        semantic = SemanticMemoryService(mock_vector_store, mock_db)
        episodic = EnhancedEpisodicMemoryService(mock_vector_store, mock_db)
        working = WorkingMemoryService(redis_url="redis://localhost:6379/1")

        retrieval = MultiTierMemoryRetrieval(
            semantic_memory=semantic,
            episodic_memory=episodic,
            working_memory=working
        )

        metrics = PerformanceMetrics("multi_tier_retrieval")

        # æ‰§è¡Œ100æ¬¡æ£€ç´¢
        for i in range(100):
            @measure_performance(metrics)
            async def retrieve_op():
                await retrieval.retrieve(
                    query=f"æµ‹è¯•æŸ¥è¯¢ {i}",
                    session_id=f"session-{i}",
                    context={"task_type": "test"}
                )
            await retrieve_op()

        stats = metrics.get_stats()
        print(f"\nğŸ“Š è·¨å±‚æ£€ç´¢æ€§èƒ½:")
        print(f"  å¹³å‡: {stats['avg']*1000:.2f}ms")
        print(f"  P95: {stats['p95']*1000:.2f}ms")
        print(f"  P99: {stats['p99']*1000:.2f}ms")

        # æ–­è¨€ï¼šå¹³å‡æ£€ç´¢æ—¶é—´åº”è¯¥ < 200ms
        assert stats['avg'] < 0.2
        print("  âœ… è·¨å±‚æ£€ç´¢æ€§èƒ½æµ‹è¯•é€šè¿‡")


# ========== è¿›åŒ–ç³»ç»Ÿæ€§èƒ½æµ‹è¯• ==========


@pytest.mark.asyncio
class TestEvolutionPerformance:
    """è¿›åŒ–ç³»ç»Ÿæ€§èƒ½æµ‹è¯•"""

    async def test_feedback_submit_performance(self):
        """æµ‹è¯•åé¦ˆæäº¤æ€§èƒ½"""
        mock_db = Mock()
        mock_episodic = Mock()
        mock_episodic.update_rating = AsyncMock(return_value=True)

        feedback_pipeline = FeedbackPipeline(mock_db, mock_episodic)

        metrics = PerformanceMetrics("feedback_submit")

        # æäº¤100æ¡åé¦ˆ
        for i in range(100):
            @measure_performance(metrics)
            async def submit_op():
                from src.core.evolution import UserFeedback
                feedback = UserFeedback(
                    episode_id=f"episode-{i}",
                    rating=5,
                    comment="æµ‹è¯•åé¦ˆ"
                )
                await feedback_pipeline.submit_feedback(feedback)
            await submit_op()

        stats = metrics.get_stats()
        print(f"\nğŸ“Š åé¦ˆæäº¤æ€§èƒ½:")
        print(f"  å¹³å‡: {stats['avg']*1000:.2f}ms")
        print(f"  P95: {stats['p95']*1000:.2f}ms")

        # æ–­è¨€ï¼šå¹³å‡æäº¤æ—¶é—´åº”è¯¥ < 50ms
        assert stats['avg'] < 0.05
        print("  âœ… åé¦ˆæäº¤æ€§èƒ½æµ‹è¯•é€šè¿‡")

    async def test_pattern_extraction_performance(self):
        """æµ‹è¯•æ¨¡å¼æå–æ€§èƒ½"""
        mock_db = Mock()
        mock_vector_store = Mock()

        extractor = ExperienceExtractor(mock_db, mock_vector_store)

        # Mock æ•°æ®åº“æŸ¥è¯¢
        mock_db.query = Mock()
        mock_db.filter = Mock()
        mock_db.all = Mock(
            return_value=[
                {
                    "episode_id": f"ep-{i}",
                    "task_type": "test",
                    "agents_involved": ["AgentA", "AgentB"],
                    "execution_trace": {"agent_sequence": ["AgentA", "AgentB"]},
                    "user_rating": 5
                }
                for i in range(50)
            ]
        )

        metrics = PerformanceMetrics("pattern_extraction")

        # æå–50æ¬¡
        for _ in range(50):
            @measure_performance(metrics)
            async def extract_op():
                await extractor.extract_from_success_cases(
                    task_type="test",
                    min_rating=4
                )
            await extract_op()

        stats = metrics.get_stats()
        print(f"\nğŸ“Š æ¨¡å¼æå–æ€§èƒ½:")
        print(f"  å¹³å‡: {stats['avg']*1000:.2f}ms")
        print(f"  P95: {stats['p95']*1000:.2f}ms")

        # æ–­è¨€ï¼šå¹³å‡æå–æ—¶é—´åº”è¯¥ < 100ms
        assert stats['avg'] < 0.1
        print("  âœ… æ¨¡å¼æå–æ€§èƒ½æµ‹è¯•é€šè¿‡")

    async def test_policy_optimization_performance(self):
        """æµ‹è¯•ç­–ç•¥ä¼˜åŒ–æ€§èƒ½"""
        mock_db = Mock()
        mock_vector_store = Mock()
        mock_vector_store.search = AsyncMock(
            return_value=[
                {
                    "agents_involved": ["AgentA", "AgentB"],
                    "user_rating": 5,
                    "similarity_score": 0.9
                }
            ]
        )

        optimizer = PolicyOptimizer(mock_db, mock_vector_store)

        metrics = PerformanceMetrics("policy_optimization")

        # ä¼˜åŒ–50æ¬¡
        for i in range(50):
            @measure_performance(metrics)
            async def optimize_op():
                await optimizer.optimize_agent_selection(
                    task_description=f"æµ‹è¯•ä»»åŠ¡ {i}",
                    task_type="test"
                )
            await optimize_op()

        stats = metrics.get_stats()
        print(f"\nğŸ“Š ç­–ç•¥ä¼˜åŒ–æ€§èƒ½:")
        print(f"  å¹³å‡: {stats['avg']*1000:.2f}ms")
        print(f"  P95: {stats['p95']*1000:.2f}ms")

        # æ–­è¨€ï¼šå¹³å‡ä¼˜åŒ–æ—¶é—´åº”è¯¥ < 150ms
        assert stats['avg'] < 0.15
        print("  âœ… ç­–ç•¥ä¼˜åŒ–æ€§èƒ½æµ‹è¯•é€šè¿‡")


# ========== å‹åŠ›æµ‹è¯• ==========


@pytest.mark.asyncio
class TestStress:
    """å‹åŠ›æµ‹è¯•"""

    async def test_concurrent_memory_operations(self):
        """æµ‹è¯•å¹¶å‘è®°å¿†æ“ä½œ"""
        mock_vector_store = Mock()
        mock_vector_store.add_documents = AsyncMock(return_value=1)

        episodic = EnhancedEpisodicMemoryService(mock_vector_store, Mock())

        metrics = PerformanceMetrics("concurrent_operations")

        # å¹¶å‘æ·»åŠ 1000æ¡æƒ…æ™¯
        tasks = []
        for i in range(1000):
            @measure_performance(metrics)
            async def add_op(idx=i):
                await episodic.add_episode(
                    session_id=f"session-{idx}",
                    task_description=f"æµ‹è¯•ä»»åŠ¡ {idx}",
                    task_type="stress_test",
                    agents_involved=["TestAgent"],
                    execution_trace={"agent_sequence": ["TestAgent"]},
                    result_summary="æµ‹è¯•ç»“æœ",
                    user_rating=5
                )
            tasks.append(add_op())

        start = time.time()
        await asyncio.gather(*tasks)
        total_time = time.time() - start

        stats = metrics.get_stats()
        print(f"\nğŸ“Š å¹¶å‘æ“ä½œå‹åŠ›æµ‹è¯•:")
        print(f"  æ€»è€—æ—¶: {total_time:.2f}s")
        print(f"  ååé‡: {1000/total_time:.2f} ops/s")
        print(f"  å¹³å‡å“åº”: {stats['avg']*1000:.2f}ms")

        # æ–­è¨€ï¼šååé‡åº”è¯¥ > 100 ops/s
        assert 1000 / total_time > 100
        print("  âœ… å¹¶å‘æ“ä½œå‹åŠ›æµ‹è¯•é€šè¿‡")


# è¿è¡ŒåŸºå‡†æµ‹è¯•
async def run_benchmarks():
    """è¿è¡Œæ‰€æœ‰åŸºå‡†æµ‹è¯•"""
    print("=" * 60)
    print("ğŸš€ å¼€å§‹æ€§èƒ½åŸºå‡†æµ‹è¯•")
    print("=" * 60)

    # è¿™é‡Œå¯ä»¥è¿è¡Œå„ç±»æµ‹è¯•
    print("\nğŸ“‹ æµ‹è¯•åˆ—è¡¨:")
    print("  1. ç¼“å­˜æ€§èƒ½æµ‹è¯•")
    print("  2. è®°å¿†ç³»ç»Ÿæ€§èƒ½æµ‹è¯•")
    print("  3. è¿›åŒ–ç³»ç»Ÿæ€§èƒ½æµ‹è¯•")
    print("  4. å‹åŠ›æµ‹è¯•")

    print("\n" + "=" * 60)
    print("âœ… åŸºå‡†æµ‹è¯•é…ç½®å®Œæˆ")
    print("  ä½¿ç”¨ pytest è¿è¡Œå®Œæ•´æµ‹è¯•:")
    print("  pytest tests/test_performance_benchmark.py -v")
    print("=" * 60)


if __name__ == "__main__":
    asyncio.run(run_benchmarks())
