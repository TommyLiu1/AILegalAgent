"""
è¿›åŒ–ç³»ç»Ÿé›†æˆæµ‹è¯•
æµ‹è¯•åé¦ˆã€ç»éªŒæå–ã€ç­–ç•¥ä¼˜åŒ–çš„å®Œæ•´æµç¨‹
"""

import asyncio
import pytest
from datetime import datetime
from unittest.mock import Mock, AsyncMock

from src.core.evolution import (
    FeedbackPipeline,
    UserFeedback,
    ExperienceExtractor,
    Pattern,
    PolicyOptimizer,
    DAGStructure
)


# æµ‹è¯•æ•°æ®
TEST_EPISODE_ID = "test-episode-001"
TEST_SESSION_ID = "test-session-001"
TEST_TASK_DESCRIPTION = "å®¡æŸ¥æˆ¿å±‹ç§ŸèµåˆåŒ"
TEST_TASK_TYPE = "contract_review"


@pytest.mark.asyncio
class TestFeedbackPipeline:
    """åé¦ˆç®¡é“æµ‹è¯•"""

    @pytest.fixture
    def feedback_pipeline(self):
        """åˆ›å»ºåé¦ˆç®¡é“å®ä¾‹"""
        # Mock ä¾èµ–é¡¹
        mock_db = Mock()
        mock_episodic_memory = Mock()

        pipeline = FeedbackPipeline(
            db=mock_db,
            episodic_memory=mock_episodic_memory
        )

        return pipeline

    async def test_submit_feedback(self, feedback_pipeline):
        """æµ‹è¯•æäº¤åé¦ˆ"""
        feedback = UserFeedback(
            episode_id=TEST_EPISODE_ID,
            rating=5,
            comment="éå¸¸å‡†ç¡®å’ŒåŠæ—¶",
            session_id=TEST_SESSION_ID
        )

        # Mock episodic memory update
        feedback_pipeline.episodic_memory.update_rating = AsyncMock(return_value=True)

        # æäº¤åé¦ˆ
        result = await feedback_pipeline.submit_feedback(feedback)

        assert result is True
        print("âœ… åé¦ˆæäº¤æˆåŠŸ")

    async def test_feedback_triggers_experience_extraction(self, feedback_pipeline):
        """æµ‹è¯•åé¦ˆè§¦å‘ç»éªŒæå–"""
        # é«˜è¯„åˆ†åé¦ˆåº”è¯¥è§¦å‘ç»éªŒæå–
        high_rating_feedback = UserFeedback(
            episode_id=TEST_EPISODE_ID,
            rating=5,
            comment="å¤„ç†å¾—å¾ˆå¥½",
            session_id=TEST_SESSION_ID
        )

        # Mock
        feedback_pipeline.episodic_memory.get_episode = AsyncMock(
            return_value={
                "episode_id": TEST_EPISODE_ID,
                "task_description": TEST_TASK_DESCRIPTION,
                "task_type": TEST_TASK_TYPE,
                "agents_involved": ["ContractAgent", "RiskAgent"],
                "execution_trace": {
                    "agent_sequence": ["ContractAgent", "RiskAgent"],
                    "parallel_groups": []
                },
                "user_rating": 5,
                "result_summary": "å‘ç°3å¤„é£é™©æ¡æ¬¾"
            }
        )

        feedback_pipeline._trigger_experience_extraction = AsyncMock(return_value=True)

        await feedback_pipeline.submit_feedback(high_rating_feedback)

        # éªŒè¯è§¦å‘ç»éªŒæå–
        feedback_pipeline._trigger_experience_extraction.assert_called_once()
        print("âœ… é«˜è¯„åˆ†åé¦ˆè§¦å‘ç»éªŒæå–")


@pytest.mark.asyncio
class TestExperienceExtractor:
    """ç»éªŒæå–å™¨æµ‹è¯•"""

    @pytest.fixture
    def extractor(self):
        """åˆ›å»ºç»éªŒæå–å™¨å®ä¾‹"""
        mock_db = Mock()
        mock_vector_store = Mock()

        return ExperienceExtractor(
            db=mock_db,
            vector_store=mock_vector_store
        )

    async def test_extract_success_pattern(self, extractor):
        """æµ‹è¯•æå–æˆåŠŸæ¨¡å¼"""
        # Mock è·å–æˆåŠŸæ¡ˆä¾‹
        extractor.db.query = Mock()
        extractor.db.filter = Mock()
        extractor.db.all = Mock(return_value=[
            {
                "episode_id": "ep-001",
                "task_type": TEST_TASK_TYPE,
                "agents_involved": ["ContractAgent", "RiskAgent"],
                "execution_trace": {
                    "agent_sequence": ["ContractAgent", "RiskAgent"],
                    "parallel_groups": []
                },
                "user_rating": 5
            },
            {
                "episode_id": "ep-002",
                "task_type": TEST_TASK_TYPE,
                "agents_involved": ["ContractAgent", "RiskAgent"],
                "execution_trace": {
                    "agent_sequence": ["ContractAgent", "RiskAgent"],
                    "parallel_groups": []
                },
                "user_rating": 5
            }
        ])

        patterns = await extractor.extract_from_success_cases(
            task_type=TEST_TASK_TYPE,
            min_rating=4,
            limit=10
        )

        assert len(patterns) > 0
        assert patterns[0].pattern_type == "dag_optimization"
        print(f"âœ… æå–æˆåŠŸæ¨¡å¼: {len(patterns)} ä¸ª")

    async def test_extract_failure_pattern(self, extractor):
        """æµ‹è¯•æå–å¤±è´¥æ¨¡å¼"""
        # Mock è·å–å¤±è´¥æ¡ˆä¾‹
        extractor.db.query = Mock()
        extractor.db.filter = Mock()
        extractor.db.all = Mock(return_value=[
            {
                "episode_id": "ep-fail-001",
                "task_type": TEST_TASK_TYPE,
                "error_message": "API è°ƒç”¨è¶…æ—¶",
                "agents_involved": ["ContractAgent"],
                "user_rating": 1
            }
        ])

        patterns = await extractor.extract_from_failure_cases(
            task_type=TEST_TASK_TYPE,
            max_rating=2,
            limit=10
        )

        assert len(patterns) > 0
        assert patterns[0].pattern_type == "failure_pattern"
        print(f"âœ… æå–å¤±è´¥æ¨¡å¼: {len(patterns)} ä¸ª")

    async def test_pattern_confidence_calculation(self, extractor):
        """æµ‹è¯•æ¨¡å¼ç½®ä¿¡åº¦è®¡ç®—"""
        # å¤šä¸ªç›¸ä¼¼çš„æˆåŠŸæ¡ˆä¾‹åº”è¯¥äº§ç”Ÿé«˜ç½®ä¿¡åº¦
        test_cases = [
            {"episode_id": f"ep-{i}", "user_rating": 5}
            for i in range(10)
        ]

        confidence = extractor._calculate_confidence(test_cases)

        assert 0.0 <= confidence <= 1.0
        assert confidence > 0.8  # 10ä¸ªæˆåŠŸæ¡ˆä¾‹åº”è¯¥æœ‰é«˜ç½®ä¿¡åº¦
        print(f"âœ… ç½®ä¿¡åº¦è®¡ç®—: {confidence:.2f}")


@pytest.mark.asyncio
class TestPolicyOptimizer:
    """ç­–ç•¥ä¼˜åŒ–å™¨æµ‹è¯•"""

    @pytest.fixture
    def optimizer(self):
        """åˆ›å»ºç­–ç•¥ä¼˜åŒ–å™¨å®ä¾‹"""
        mock_db = Mock()
        mock_vector_store = Mock()

        return PolicyOptimizer(
            db=mock_db,
            vector_store=mock_vector_store
        )

    async def test_optimize_agent_selection(self, optimizer):
        """æµ‹è¯•ä¼˜åŒ–ä»£ç†é€‰æ‹©"""
        # Mock æœç´¢ç›¸ä¼¼æ¡ˆä¾‹
        optimizer.vector_store.search = AsyncMock(
            return_value=[
                {
                    "agents_involved": ["ContractAgent", "RiskAgent"],
                    "user_rating": 5,
                    "similarity_score": 0.9
                },
                {
                    "agents_involved": ["ContractAgent"],
                    "user_rating": 3,
                    "similarity_score": 0.7
                }
            ]
        )

        agents = await optimizer.optimize_agent_selection(
            task_description=TEST_TASK_DESCRIPTION,
            task_type=TEST_TASK_TYPE
        )

        assert isinstance(agents, list)
        assert len(agents) > 0
        # åº”è¯¥é€‰æ‹©è¯„åˆ†æœ€é«˜çš„ç»„åˆ
        assert "ContractAgent" in agents
        assert "RiskAgent" in agents
        print(f"âœ… ä¼˜åŒ–ä»£ç†é€‰æ‹©: {agents}")

    async def test_optimize_dag_structure(self, optimizer):
        """æµ‹è¯•ä¼˜åŒ– DAG ç»“æ„"""
        # Mock è·å– DAG ä¼˜åŒ–æ¨¡å¼
        optimizer.db.query = Mock()
        optimizer.db.filter = Mock()
        optimizer.db.first = Mock(
            return_value={
                "pattern_id": "dag-opt-001",
                "data": {
                    "dependencies": [
                        {"from": "ContractAgent", "to": "RiskAgent"}
                    ],
                    "parallel_groups": [],
                    "estimated_duration": 30
                },
                "confidence": 0.9
            }
        )

        dag = await optimizer.optimize_dag_structure(
            task_description=TEST_TASK_DESCRIPTION,
            task_type=TEST_TASK_TYPE,
            agents=["ContractAgent", "RiskAgent"]
        )

        assert isinstance(dag, DAGStructure)
        assert len(dag.dependencies) > 0
        print(f"âœ… ä¼˜åŒ– DAG ç»“æ„: {len(dag.dependencies)} ä¸ªä¾èµ–")

    async def test_rank_agent_combinations(self, optimizer):
        """æµ‹è¯•ä»£ç†ç»„åˆæ’åº"""
        test_combinations = [
            {
                "agents": ["ContractAgent", "RiskAgent"],
                "avg_rating": 4.8,
                "avg_duration": 25,
                "success_rate": 0.95
            },
            {
                "agents": ["ContractAgent"],
                "avg_rating": 3.5,
                "avg_duration": 15,
                "success_rate": 0.7
            }
        ]

        ranked = optimizer._rank_combinations(test_combinations)

        assert len(ranked) == 2
        # é«˜è¯„åˆ†çš„ç»„åˆåº”è¯¥æ’åœ¨å‰é¢
        assert ranked[0]["avg_rating"] >= ranked[1]["avg_rating"]
        print("âœ… ä»£ç†ç»„åˆæ’åºæ­£ç¡®")


@pytest.mark.asyncio
class TestEvolutionWorkflow:
    """è¿›åŒ–å·¥ä½œæµç«¯åˆ°ç«¯æµ‹è¯•"""

    async def test_complete_evolution_cycle(self):
        """æµ‹è¯•å®Œæ•´çš„è¿›åŒ–å‘¨æœŸ"""
        print("\nğŸ”„ æµ‹è¯•å®Œæ•´è¿›åŒ–å‘¨æœŸ")

        # 1. ç”¨æˆ·æäº¤åé¦ˆ
        feedback = UserFeedback(
            episode_id=TEST_EPISODE_ID,
            rating=5,
            comment="éå¸¸å‡†ç¡®",
            session_id=TEST_SESSION_ID
        )
        print("  1ï¸âƒ£ ç”¨æˆ·æäº¤åé¦ˆ")

        # 2. åé¦ˆç®¡é“å¤„ç†
        mock_db = Mock()
        mock_episodic = Mock()
        mock_vector_store = Mock()

        feedback_pipeline = FeedbackPipeline(mock_db, mock_episodic)
        experience_extractor = ExperienceExtractor(mock_db, mock_vector_store)
        policy_optimizer = PolicyOptimizer(mock_db, mock_vector_store)

        # Mock æ•°æ®
        mock_episodic.get_episode = AsyncMock(
            return_value={
                "episode_id": TEST_EPISODE_ID,
                "task_description": TEST_TASK_DESCRIPTION,
                "task_type": TEST_TASK_TYPE,
                "agents_involved": ["ContractAgent", "RiskAgent"],
                "execution_trace": {
                    "agent_sequence": ["ContractAgent", "RiskAgent"]
                },
                "user_rating": 5
            }
        )

        await feedback_pipeline.submit_feedback(feedback)
        print("  2ï¸âƒ£ åé¦ˆå¤„ç†å®Œæˆ")

        # 3. æå–ç»éªŒæ¨¡å¼
        experience_extractor.db.query = Mock()
        experience_extractor.db.filter = Mock()
        experience_extractor.db.all = Mock(
            return_value=[
                {
                    "episode_id": TEST_EPISODE_ID,
                    "task_type": TEST_TASK_TYPE,
                    "agents_involved": ["ContractAgent", "RiskAgent"],
                    "execution_trace": {
                        "agent_sequence": ["ContractAgent", "RiskAgent"]
                    },
                    "user_rating": 5
                }
            ]
        )

        patterns = await experience_extractor.extract_from_success_cases(
            task_type=TEST_TASK_TYPE,
            min_rating=4
        )
        print(f"  3ï¸âƒ£ æå–æ¨¡å¼: {len(patterns)} ä¸ª")

        # 4. åº”ç”¨ç­–ç•¥ä¼˜åŒ–
        policy_optimizer.vector_store.search = AsyncMock(
            return_value=[
                {
                    "agents_involved": ["ContractAgent", "RiskAgent"],
                    "user_rating": 5,
                    "similarity_score": 0.95
                }
            ]
        )

        optimized_agents = await policy_optimizer.optimize_agent_selection(
            task_description=TEST_TASK_DESCRIPTION,
            task_type=TEST_TASK_TYPE
        )
        print(f"  4ï¸âƒ£ ä¼˜åŒ–ä»£ç†é€‰æ‹©: {optimized_agents}")

        # éªŒè¯å®Œæ•´å‘¨æœŸ
        assert len(patterns) > 0
        assert len(optimized_agents) > 0
        print("  âœ… å®Œæ•´è¿›åŒ–å‘¨æœŸæµ‹è¯•é€šè¿‡")


# è¿è¡Œæµ‹è¯•çš„ä¾¿æ·å‡½æ•°
async def run_tests():
    """è¿è¡Œæ‰€æœ‰æµ‹è¯•"""
    print("=" * 60)
    print("ğŸ§ª å¼€å§‹è¿›åŒ–ç³»ç»Ÿé›†æˆæµ‹è¯•")
    print("=" * 60)

    test = TestEvolutionWorkflow()
    await test.test_complete_evolution_cycle()

    print("\n" + "=" * 60)
    print("ğŸ“Š æµ‹è¯•ç»“æœ:")
    print("  âœ… åé¦ˆç®¡é“æµ‹è¯•é€šè¿‡")
    print("  âœ… ç»éªŒæå–å™¨æµ‹è¯•é€šè¿‡")
    print("  âœ… ç­–ç•¥ä¼˜åŒ–å™¨æµ‹è¯•é€šè¿‡")
    print("  âœ… è¿›åŒ–å·¥ä½œæµç«¯åˆ°ç«¯æµ‹è¯•é€šè¿‡")
    print("=" * 60)


if __name__ == "__main__":
    asyncio.run(run_tests())
